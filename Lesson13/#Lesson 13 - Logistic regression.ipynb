{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"#Lesson 13 - Logistic regression.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"fDzsloXeJE3f","toc-hr-collapsed":false},"source":["# 1.0 Classification\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"fqkGKXzvJFv5"},"source":["\n","In the previous lessons, we explored a supervised machine learning technique called **linear regression**. Linear regression works well when the target column we're trying to predict, the dependent variable, is ordered and continuous. **If the target column instead contains discrete values, then linear regression isn't a good fit**.\n","\n","In this lesson, we'll explore how to build a predictive model for these types of problems, which are known as **classification problems**. In classification, our target column has a finite set of possible values which represent different categories a row can belong to. We use integers to represent the different categories so we can continue to use mathematical functions to describe how the independent variables map to the dependent variable. Here are a few examples of classification problems:\n","\n","| Problem                                                                   | Sample Features                                    | Type        | Categories           | Numerical Categories |\n","|---------------------------------------------------------------------------|----------------------------------------------------|-------------|----------------------|----------------------|\n","| Should we accept this student based on their graduate school application? | College GPA, SAT Score, Quality of Recommendations | Binary      | Don't Accept, Accept | 0, 1                 |\n","| What is the most likely blood type of 2 parent's offspring?               | Parent 1's blood type, Parent 2's blood type.      | Multi-class | A, B, AB, O          | 1, 2, 3, 4           |\n","\n","\n","We'll focus on **binary classification** for now, where the only 2 options for values are:\n","\n","- **0** for the False condition,\n","- **1** for the True condition.\n","\n","Before we dive into classification, let's understand the data we'll be working with."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"2wMlyd4AITqv"},"source":["## 1.1 Introduction to the data\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"MW5u3eDhNRD1"},"source":["\n","\n","Every year high school student and college students apply to various universities and programs. Each student has a unique set of test scores, grades, and backgrounds. Somehow, the admission commitees must decide which applicants to accept or reject from their program. In this case a binary classification algorithm can be used to accept or reject applicants. To grasp the concepts of classification, we'll try to predict whether an applicant will be admitted to a graduate program in a U.S. university.\n","\n","We'll be working with a dataset containing data on 644 applicants with the following columns:\n","\n","- **gre** - applicant's score on the Graduate Record Exam, a generalized test for prospective graduate students.\n","    - Score ranges from 200 to 800.\n","- **gpa** - college grade point average.\n","    - Continuous between 0.0 and 4.0.\n","- **admit** - binary value\n","    - Binary value, 0 or 1, where 1 means the applicant was admitted to the program and 0 means the applicant was rejected.\n","\n","Here's a preview of the dataset:\n","\n","| admit | gpa      | gre        |\n","|-------|----------|------------|\n","| 0     | 3.177277 | 594.102992 |\n","| 0     | 3.412655 | 631.528607 |\n","| 0     | 2.728097 | 553.714399 |\n","| 0     | 3.093559 | 551.089985 |\n","| 0     | 3.141923 | 537.184894 |\n","\n","\n","While we can use both the **gpa** and **gre** columns to predict the **admit** column, we'll focus on using just the **gpa** column to keep things simple. Let's read the data into Pandas and visualize the relationship between **gpa** and **admit**.\n","\n","\n","**Exercise Start**\n","\n","<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n","\n","\n","- Read **admissions.csv** into a Dataframe named **admissions**.\n","- Use the Matplotlib method [scatter](http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.scatter) to generate a scatter plot with the:\n","  - **gpa** column on the x-axis.\n","  - **admit** column on the y-axis.\n","- Use **plt.show()** to display the scatter plot."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"UgrKdj-OPk5y","colab":{}},"source":["# put your code here"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"WyHKyDYIITq3"},"source":["## 1.2 Logistic regression\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"QTp1elcUP_qp"},"source":["\n","\n","In the previous scatter plot, you'll notice that the **gpa** column and the **admit** column do not have a clear linear relationship. Recall that the **admit** column only contains the values 0 and 1 and are used to represent binary values and the numbers themselves don't carry any weight. When numbers are used to represent different options or categories, they are referred to as **categorical values**. Classification focuses on estimating the relationship between the independent variables (x) and the dependent (y), **categorical variable**.\n","\n","In this lesson, we'll focus on a classification technique called **logistic regression**. While a **linear regression** model outputs a real number as the label, a **logistic regression** model outputs a probability value. In binary classification, if the probability value is larger than a certain threshold probability, we assign the label for that row to 1 or 0 otherwise.\n","\n","This threshold probability is something we select, and we'll learn about how to select a good threshold probability in later classes. For now, let's dive more into how logistic regression works."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"2S3hN_IBITq4"},"source":["## 1.3 Logistic function\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"wLg8sv2yfLzh"},"source":["\n","\n","In **linear regression**, we used the linear function $y = mx + b$ to represent the relationship between the independent variables and the dependent variable. In logistic regression, we use the **logistic function**, which is a version of the linear function that is adapted for classification.\n","\n","Let's explore some of the **logistic function's** properties to better understand why it's useful for **classification** tasks. Unlike in linear regression, where the output can be any real value, in **logistic regression** the output has to be a real value between 0 and 1, since the output represents a probability value. Note that the model can't output a negative value or it would violate this criteria.\n","\n","Here's the mathematical representation of the logistic function:\n","\n","$$\\sigma(t)=\\frac{1}{1+e^{-t}}$$\n","\n","In the following code cell, we plotted the **logistic function** to visualize its properties. Specifically, we:\n","\n","- define the **logistic()** function using the [NumPy exp](http://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.exp.html) function,\n","- generate equally spaced values, between **-6** and **6** to represent the x-axis,\n","- calculate the y-axis values by feeding each value in x to the **logistic()** function,\n","- creating a line plot to visualize x and y.\n"]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1566317247221,"user_tz":180,"elapsed":1343,"user":{"displayName":"Ivanovitch Silva","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBcjji7pCNFgk53T2rHmLlR9pgrTAc60gzJQYMZ2A=s64","userId":"06428777505436195303"}},"id":"k2dqahgyITq5","outputId":"a28c77b0-cd54-4567-874b-e382ddf3f8c0","colab":{"base_uri":"https://localhost:8080/","height":284}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# Logistic Function\n","def logistic(x):\n","    # np.exp(x) raises x to the exponential power, ie e^x. e ~= 2.71828\n","    return 1 / (1 + np.exp(-x)) \n","    \n","# Generate 50 real values, evenly spaced, between -6 and 6.\n","x = np.linspace(-6,6,50, dtype=float)\n","\n","# Transform each number in t using the logistic function.\n","y = logistic(x)\n","\n","# Plot the resulting data.\n","plt.plot(x, y)\n","plt.ylabel(\"Probability\")\n","plt.xlabel(\"x\")\n","plt.show()"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKgUREWmmUhARkWYqBRERafb/AQa/l5sU2VO+AAAAAElFTkSuQmCC\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"GK2U83O6ITq8"},"source":["## 1.4 Training a logistic regression model\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"9J3J8YMt-91T"},"source":["\n","\n","Let's now move onto training the logistic regression model using our dataset. We won't dive into the math and the steps required to fit a logistic regression model to the training data in this pratice. We'll instead focus on using the **scikit-learn** library to fit a model between the **gpa** and **admit** columns. Recall that the **gpa** column contains the GPA of each applicant as a real value between **0.0** and **4.0** and the **admit** column specifies if that applicant was admitted (0 if not admitted and 1 if admitted). Since we're only working with one feature, **gpa**, this is referred to as a univariate model.\n","\n","Training a **logistic regression** model in **scikit-learn** is similar to training a **linear regression** model, with the key difference that we use the **LogisticRegression class** instead of the **LinearRegression class**. Scikit-learn was designed to make it easy to swap out models by keeping the syntax and notation as consistent as possible across it's different classes.\n","\n","\n","**Exercise Start**\n","\n","<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n","\n","\n","\n","- Import the **LogisticRegression class** and instantiate a model named **logistic_model**.\n","- Use the LogisticRegression method **fit** to fit the model to the data. We're only interested in constructing a model that uses **gpa** values to predict **admit** values.\n","- View the [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) for the **LogisticRegression class** if you get stuck.\n","- Tip: use df[[\"gpa\"]] to guarantee a shape of (m,1)"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"JCQgfgfSAKdi","colab":{}},"source":["# put your code here"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"wE1B6LGUITrB"},"source":["## 1.5 Plotting probabilities\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Z-znddS5AeLk"},"source":["\n","\n","We mentioned earlier that the output of a logistic regression model is the probability that the row should be labelled as **True**, or in our case **1**. We can use the trained model to return the predicted probability for each row in the training data.\n","\n","To return the predicted probability, use the [predict_proba method](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.predict_proba). The only required parameter for this method is the **num_features** by **num_sample** matrix of observations we want scikit-learn to return predicted probabilities for. For each input row, scikit-learn will return a NumPy array with 2 probability values:\n","\n","- the probability that the row should be labelled 0,\n","- the probability that the row should be labelled 1.\n","\n","Since 0 and 1 are the only 2 possible categories and represent the entire outcome space, these 2 probabilities will always add upto 1.\n","\n","```python\n","probabilities = logistic_model.predict_proba(admissions[[\"gpa\"]])\n","# Probability that the row belongs to label `0`.\n","probabilities[:,0]\n","# Probabililty that the row belongs to label `1`.\n","probabilities[:,1]\n","```\n","\n","Let's use this method to return the probability for each student that they would be admitted and then visualize the results on a scatter plot.\n","\n","\n","**Exercise Start**\n","\n","<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n","\n","\n","- Use the **LogisticRegression** method **predict_proba** to return the predicted probabilities for the data in the **gpa** column. Assign the returned probabilities to **pred_probs**.\n","\n","- Create and display a scatter plot using the [Matplotlib scatter](http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.scatter) function where:\n","  - the x-axis is the values in the **gpa** column,\n","  - the y-axis is the **probability** of being classified as label 1.\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"B-R8imA3Cc2x","colab":{}},"source":["# put your code here"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"-ddVEF6eITrF"},"source":["## 1.6 Predict labels\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"GSqKRALwC1TD"},"source":["\n","You'll notice that the **scatter plot** suggests a **linear relationship** between the **gpa** values and the **probability** of being admitted. This is because logistic regression is really just an adapted version of linear regression for classification problems. Both logistic and linear regression are used to capture linear relationships between the independent variables and the dependent variable.\n","\n","Let's now use the **predict** method to return the label predictions for each row in our training dataset.\n","\n","**Exercise Start**\n","\n","<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n","\n","- Use the **LogisticRegression** method [predict](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.predict) to return the predicted for each label in the training set.\n","  - The parameter for the **predict** method matches that of the **predict_proba** method:\n","    - X: rows of data to use for prediction.\n","  - Assign the result to **fitted_labels.**\n","-  Create and display a scatter plot using the [Matplotlib scatter](http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.scatter) function where:\n","  - the x-axis is the values in the **gpa** column,\n","  - the y-axis is the **fitted_labels** variable.\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Zjcg2QInDqbV","colab":{}},"source":["# put your code here"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"mNAMPvaKITrK","toc-hr-collapsed":false},"source":["# 2.0 Introduction to evaluating binary classifiers\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"HB8Gm2fvLdmY"},"source":["\n","In the previous section, we learned about **classification**, logistic regression, and how to use **scikit-learn** to fit a **logistic regression** model to a dataset on graduate school admissions. We'll continue to work with the dataset.\n","\n","\n","**Exercise Start**\n","\n","<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n","\n","\n","\n","- Use the **LogisticRegression** method **predict** to return the label for each observation in the dataset, **admissions**. Assign the returned list to **labels**.\n","- Add a new column to the **admissions** Dataframe named **predicted_label** that contains the values from **labels**.\n","- Use the **Series** method **value_counts** and the print function to display the distribution of the values in the **predicted_label** column.\n","- Use the **Dataframe** method **head** and the print function to display the first 5 rows in **admissions**."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"1Rj7vvFfGfmQ","colab":{}},"source":["# put your ocde here"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"F0lz2_SPGxTJ"},"source":["## 2.1 Accuracy\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ohnI2nxELxDy"},"source":["\n","The **admissions** Dataframe now contains the predicted value for that row, in the **predicted_label** column, and the actual value for that row, in the **admit** column. This format makes it easier for us to calculate how effective the model was on the training data. The simplest way to determine the effectiveness of a classification model is **prediction accuracy**. Accuracy helps us answer the question:\n","\n","- **What fraction of the predictions were correct (actual label matched predicted label)**?\n","\n","\n","Prediction accuracy boils down to the number of labels that were correctly predicted divided by the total number of observations:\n","\n","$$Accuracy=\\frac{\\text{#correct predictions}}{\\text{#observations}}$$\n","\n","\n","In **logistic regression**, recall that the model's output is a probability between 0 and 1. To decide who gets admitted, we set a threshold and accept all of the students where their computed probability exceeds that threshold. This threshold is called the **discrimination threshold** and **scikit-learn** sets it to 0.5 by default when predicting labels. If the predicted probability is greater than 0.5, the label for that observation is 1. If it is instead less than 0.5, the label for that observation is 0.\n","\n","An accuracy of 1.0 means that the model predicted 100% of admissions correctly for the given discrimination threshold. An accuracy of 0.2 means that the model predicted 20% of the admissions correctly. Let's calculate the accuracy for the predictions the **logistic regression** model made.\n","\n","\n","**Exercise Start**\n","\n","<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n","\n","\n","- Rename the **admit** column from the **admissions** Dataframe to **actual_label** so it's more clear which column contains the predicted labels (**predicted_label**) and which column contains the actual labels (**actual_label**).\n","- Compare the **predicted_label** column with the **actual_label** column.\n","  - Use a double equals sign (**==**) to compare the two Series objects and assign the resulting **Series** object to **matches**.\n","- Use conditional filtering to filter **admissions** to just the rows where matches is **True**. Assign the resulting Dataframe to **correct_predictions**.\n","- Display the first 5 rows in **correct_predictions** to make sure the values in the **predicted_label** and **actual_label** columns are equal.\n","- Calculate the **accuracy** and assign the resulting float value to **accuracy**.\n","- Display accuracy using the print function."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"TICnUgw9PFly","colab":{}},"source":["# put your code here"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"LYHJh_TjGxTO"},"source":["## 2.2 Binary classification outcomes\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"qwFAJG_hSxHl"},"source":["\n","\n","It looks like the raw accuracy is around **64.6%** which is better than randomly guessing the label (which would result in around a **50% accuracy**). Calculating the accuracy of a model on the dataset used for training is a useful **initial step just to make sure the model at least beats randomly** assigning a label for each observation. However, **prediction accuracy doesn't tell us much more**.\n","\n","The accuracy doesn't tell us how the model performs on data it wasn't trained on. A model that returns a 100% accuracy when evaluated on it's training set doesn't tell us how well the model works on data it's never seen before (and wasn't trained on). Accuracy also doesn't help us discriminate between the different types of outcomes a binary classification model can make. In a later section we'll learn how to evaluate a model's effectiveness on new, unseen data. In this section, we'll focus on the principles of evaluating binary classification models by testing our model's effectiveness on the training data.\n","\n","To start, let's discuss the four different outcomes of a binary classification model:\n","\n","| Prediction   | Observation       |         |\n","|--------------|---------------------|---------------------|\n","|       | |      Admitted (1)       |         Rejected (0) |\n","| Admitted (1) | True Positive (TP)  | False Positive (FP) |\n","| Rejected (0) | False Negative (FN) | True Negative (TN)  |\n","\n","\n","$\n","\\displaystyle Accuracy = \\frac{\\#correct\\_predictions}{\\#observed\\_predictions}\\\\\n","\\displaystyle Accuracy = \\frac{TP + TN}{TP + TN + FN + FP} \\\\\n","$\n","\n","By segmenting a model's predictions into these different outcome categories, we can start to think about other measures of effectiveness that give us more granularity than simple accuracy.\n","\n","We can define these outcomes as:\n","\n","- **True Positive** - The model correctly predicted that the student would be admitted.\n","\n","    - Said another way, the model predicted that the **label** would be **Positive**, and that **ended up** being **True**.\n","    - In our case, Positive refers to being admitted and maps to the label 1 in the dataset.\n","    - For this dataset, a **true positive** is whenever **predicted_label** is 1 and **actual_label** is 1.\n","- **True Negative** - The model correctly predicted that the student would be rejected.\n","\n","    - Said another way, the model predicted that the **label** would be **Negative**, and that **ended** up being **True**.\n","    - In our case, Negative refers to being rejected and maps to the label 0 in the dataset.\n","    - For this dataset, a **true negative** is whenever **predicted_label** is 0 and **actual_label** is 0.\n","- **False Positive** - The model incorrectly predicted that the student would be admitted even though the student was actually rejected.\n","\n","    - Said another way, the model predicted that the label would be Positive, but that was False (the actual label was False).\n","    - For this dataset, a false positive is whenever **predicted_label** is 1 but the **actual_label** is 0.\n","- **False Negative** - The model incorrectly predicted that the student would be rejected even though the student was actually admitted.\n","\n","    - Said another way, the model predicted that the would be Negative, but that was False (the actual value was True).\n","    - For this dataset, a false negative is whenever **predicted_label** is 0 but the **actual_label** is 1.\n","    \n","Let's calculate the number of observations that fall into some of these outcome categories.\n","\n","\n","**Exercise Start**\n","\n","<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n","\n","\n","- Extract all of the rows where **predicted_label** and **actual_label** both equal 1. Then, calculate the number of **true positives** and assign to **true_positives**.\n","- Extract all of the rows where **predicted_label** and **actual_label** both equal 0. Then, calculate the number of **true negatives** and assign to **true_negatives**.\n","- Display both **true_positives** and **true_negatives**."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"cvlAWXaAdYjY","colab":{}},"source":["# put your code here"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ErZVqMlFGxTS"},"source":["## 2.3 Sensitivity\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"EgG9TGibdspE"},"source":["\n","\n","Let's now look at a few measures that are much more insightful than simple accuracy. Let's start with **sensitivity**:\n","\n","- **Sensitivity** or **True Positive Rate** - The proportion of applicants that were correctly admitted:\n","\n","$$TPR=\\frac{\\text{#true positives}}{\\text{#true positives + #false negatives}}$$\n","\n","\n","From all the students that should have been admitted (True Positives + False Negatives), what fraction did the model correctly admit (True Positives)? More generally, this measure helps us answer the question:\n","\n","- **How effective is this model at identifying positive outcomes?**\n","\n","In our case, the positive outcome (label of 1) is admitting a student. If the True Positive Rate is low, it means that the model isn't effective at catching positive cases. **For certain problems, high sensitivity is incredibly important.** If we're building a model to predict which patients have cancer, every patient that is missed by the model could mean a loss of life. We want a **highly sensitive** model that is able to \"catch\" all of the positive cases (in this case, the positive case is a patient with cancer).\n","\n","Let's calculate the sensitivity for the model we're working with.\n","\n","**Exercise Start**\n","\n","<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n","\n","\n","- Calculate the number of **false negatives** (where the model predicted rejected but the student was actually admitted) and assign to **false_negatives**.\n","- Calculate the **sensitivity** and assign the computed value to **sensitivity**.\n","- Display **sensitivity**."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"s0ilAnoefsDk","colab":{}},"source":["# put your code here"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"4fEj13JQGxTX"},"source":["## 2.4 Specificity\n","\n","Looks like the sensitivity of the model is around **12.7%** and only about 1 in 8 students that should have been admitted were actually admitted. In the context of predicting student admissions, this probably isn't too bad of a thing. Graduate schools can only admit a select number of students into their programs and by definition they end up rejecting many qualified students that would have succeeded.\n","\n","In the healthcare context, however, low sensitivity could mean a severe loss of life. If a classification model is only catching **12.7%** of positive cases for an illness, then around 7 of 8 people are going undiagnosed (being classified as false negatives). Hopefully you're beginning to acquire a sense for the tradeoffs predictive models make and the importance of understanding the various measures.\n","\n","Let's now learn about **specificity**:\n","\n","- **Specificity** or **True Negative Rate** - The proportion of applicants that were correctly rejected:\n","\n","$$TNR=\\frac{\\text{#true negatives}}{\\text{#true negatives + #false positives}}$$\n","\n","This helps us answer the question:\n","\n","How effective is this model at identifying negative outcomes?\n","In our case, the specificity tells us the proportion of applicants who should be rejected (**actual_label** equal to 0, which consists of False Positives + True Negatives) that were correctly rejected (just True Negatives). **A high specificity means that the model is really good at predicting which applicants should be rejected**.\n","\n","Let's calculate the specificity of our model.\n","\n","**Exercise Start**\n","\n","<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n","\n","\n","- Calculate the number of **false positives** (where the model predicted admitted but the student was actually rejected) and assign to **false_positives**.\n","- Calculate the **specificity** and assign the computed value to **specificity.**\n","- Display **specificity**."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"UA2PLDLAs-DJ","colab":{}},"source":["# put your code here"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"3sjEe825GxTa"},"source":["## 2.5 Next steps\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"toODOr8AtI64"},"source":["\n","\n","It looks like the specificity of the model is **96.25%**. This means that **the model is really good at knowing which applicants to reject**. Since around only 7% of applicants were accepted that applied, it's important that the model reject people correctly who wouldn't have otherwise been accepted.\n","\n","In this mission, we learned about some of the different ways of evaluating how well a binary classification model performs. The different measures we learned about have very similar names and it's easy to confuse them. Don't fret! The important takeaway is the ability to frame the question you want to answer and working backwards from that to formulate the correct calculation.\n","\n","If you want to know how well a binary classification model is at catching positive cases, you should have the intuition to divide the correctly predicted positive cases by all actually positive cases. There are 2 outcomes associated with an admitted student (positive case), a false negative and a true positive. Therefore, by dividing the number of true positives by the sum of false negatives and true positives, you'll have the proportion corresponding to the model's effectiveness of identifying positive cases. While this proportion is referred to as the sensitivity, the word itself is secondary to the concept and being able to work backwards to the formula!\n","\n","These measures are just a starting point, however, and aren't super useful by themselves. In the next section, we'll dive into **cross-validation**, where we'll evaluate our model's accuracy on new data that it wasn't trained on. In addition, we'll explore how varying the discrimination threshold affects the measures we learned about in this lesson. These important techniques help us gain a much more complete understanding of a classification model's performance."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"NRv5iWbNuPsh","toc-hr-collapsed":false},"source":["# 3.0 Multiclass classification\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"-HyiZmNLFbfe"},"source":["## 3.1 Introduction to the data\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"xpoQ3eg4Fp4D"},"source":["The dataset we will be working with contains information on various cars. For each car we have information about the technical aspects of the vehicle such as the motor's displacement, the weight of the car, the miles per gallon, and how fast the car accelerates. Using this information we will predict the origin of the vehicle, either North America, Europe, or Asia. We can see, that unlike our previous classification datasets, we have three categories to choose from, making our task slightly more challenging.\n","\n","Here's a preview of the data:\n","\n","```python\n","18.0   8   307.0      130.0      3504.      12.0   70  1    \"chevrolet chevelle malibu\"\n","15.0   8   350.0      165.0      3693.      11.5   70  1    \"buick skylark 320\"\n","18.0   8   318.0      150.0      3436.      11.0   70  1    \"plymouth satellite\"\n","```\n","\n","The dataset is hosted by the **University of California Irvine** on their machine learning [repository](https://archive.ics.uci.edu/ml/datasets/Auto+MPG). As a side note, the UCI Machine Learning repository contains many small datasets which are useful when getting your hands dirty with machine learning.\n","\n","You'll notice that the **Data Folder** contains a few different files. We'll be working with **auto-mpg.data**, which omits the 8 rows containing missing values for fuel efficiency (mpg column). We've converted this data into a CSV file named **auto.csv** for you.\n","\n","Here are the columns in the dataset:\n","\n","- **mpg** -- Miles per gallon, Continuous.\n","- **cylinders** -- Number of cylinders in the motor, Integer, Ordinal, and Categorical.\n","- **displacement** -- Size of the motor, Continuous.\n","- **horsepower** -- Horsepower produced, Continuous.\n","- **weight** -- Weights of the car, Continuous.\n","- **acceleration** -- Acceleration, Continuous.\n","- **year** -- Year the car was built, Integer and Categorical.\n","- **origin** -- Integer and Categorical. 1: North America, 2: Europe, 3: Asia.\n","- **car_name** -- Name of the car.\n","\n","**Exercise Start**\n","\n","<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n","\n","- Import the Pandas library and read **auto.csv** into a Dataframe named **cars**.\n","- Use the **Series.unique()** method to assign the unique elements in the column **origin** to **unique_regions**. Then use the **print** function to display **unique_regions.**"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"cKKTMfzHHqr2","colab":{}},"source":["# put your code here"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"RIvksHd9Io_3"},"source":["## 3.2 Dummy Variables"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"5ZayyljpI0r_"},"source":["In previous classification sections, **categorical variables** have been represented in the dataset using **integer values** (like 0 and 1) for us already. In many cases, like with this dataset, you'll have to create numeric representation of categorical values yourself. For this dataset, **categorical variables exist** in three columns, **cylinders**, **year**, and **origin**. The **cylinders** and **year** columns must be converted to numeric values so we can use them to predict label **origin**. Even though the column **year** is a number, weâ€™re going to treat them like categories. The year 71 is unlikely to relate to the year 70 in the same way those two numbers do numerically, but rather just as two different labels. In these instances, it is always safer to treat discrete values as categorical variables.\n","\n","We must use **dummy variables** for columns containing categorical values. Whenever we have more than 2 categories, we need to create more columns to represent the categories. Since we have 5 different categories of cylinders, we could use 3, 4, 5, 6, and 8 to represent the different categories. We can split the column into separate binary columns:\n","\n","\n","- **cyl_3** -- Does the car have 3 cylinders? 0 if **False**, 1 if **True.**\n","- **cyl_4** -- Does the car have 4 cylinders? 0 if **False**, 1 if **True.**\n","- **cyl_5** -- Does the car have 5 cylinders? 0 if **False**, 1 if **True.**\n","- **cyl_6** -- Does the car have 6 cylinders? 0 if **False**, 1 if **True.**\n","- **cyl_8** -- Does the car have 8 cylinders? 0 if **False**, 1 if **True.**\n","\n","\n","We can use the [pandas.get_dummies()](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html) function to return a Dataframe containing binary columns from the values in the **cylinders** column. In addition, if we set the prefix parameter to **cyl**, Pandas will prepend the column names to match the style we'd like:\n","\n","```python\n","18.0   8   307.0      130.0      3504.      12.0   70  1    \"chevrolet chevelle malibu\"\n","15.0   8   350.0      165.0      3693.      11.5   70  1    \"buick skylark 320\"\n","18.0   8   318.0      150.0      3436.      11.0   70  1    \"plymouth satellite\"\n","dummy_df = pd.get_dummies(cars[\"cylinders\"], prefix=\"cyl\")\n","```\n","\n","We then use the [pandas.concat()](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.concat.html) function to add the columns from this Dataframe back to cars:\n","\n","```python\n","cars = pd.concat([cars, dummy_df], axis=1)\n","```\n","\n","Now it's your turn! Repeat the same process for the **year** column.\n","\n","**Exercise Start**\n","\n","<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n","\n","- Use the **pandas.get_dummies()** function to create dummy values from the **year** column.\n","  - Use the **prefix** attribute to prepend **year** to each of the resulting column names.\n","  - Assign the resulting Dataframe to **dummy_years**.\n","- Use the **pandas.concat()** function to concatenate the columns from **dummy_years** to **cars**.\n","- Use the **DataFrame.drop()** method to drop the **year** and **cylinders** columns from **cars.**\n","- Display the first 5 rows of the new cars Dataframe to confirm."]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"elapsed":825,"status":"ok","timestamp":1539517426949,"user":{"displayName":"Ivanovitch Silva","photoUrl":"","userId":"06428777505436195303"},"user_tz":180},"id":"1gca6nHCLNOA","outputId":"ec933732-5991-4ce3-c4e0-4729dd825bf3","colab":{"base_uri":"https://localhost:8080/","height":224}},"source":["dummy_cylinders = pd.get_dummies(cars[\"cylinders\"], prefix=\"cyl\")\n","cars = pd.concat([cars, dummy_cylinders], axis=1)\n","cars.head()\n","\n","# put your code here"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>mpg</th>\n","      <th>cylinders</th>\n","      <th>displacement</th>\n","      <th>horsepower</th>\n","      <th>weight</th>\n","      <th>acceleration</th>\n","      <th>year</th>\n","      <th>origin</th>\n","      <th>cyl_3</th>\n","      <th>cyl_4</th>\n","      <th>cyl_5</th>\n","      <th>cyl_6</th>\n","      <th>cyl_8</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>18.0</td>\n","      <td>8</td>\n","      <td>307.0</td>\n","      <td>130.0</td>\n","      <td>3504.0</td>\n","      <td>12.0</td>\n","      <td>70</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>15.0</td>\n","      <td>8</td>\n","      <td>350.0</td>\n","      <td>165.0</td>\n","      <td>3693.0</td>\n","      <td>11.5</td>\n","      <td>70</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>18.0</td>\n","      <td>8</td>\n","      <td>318.0</td>\n","      <td>150.0</td>\n","      <td>3436.0</td>\n","      <td>11.0</td>\n","      <td>70</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>16.0</td>\n","      <td>8</td>\n","      <td>304.0</td>\n","      <td>150.0</td>\n","      <td>3433.0</td>\n","      <td>12.0</td>\n","      <td>70</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>17.0</td>\n","      <td>8</td>\n","      <td>302.0</td>\n","      <td>140.0</td>\n","      <td>3449.0</td>\n","      <td>10.5</td>\n","      <td>70</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    mpg  cylinders  displacement  horsepower  weight  acceleration  year  \\\n","0  18.0          8         307.0       130.0  3504.0          12.0    70   \n","1  15.0          8         350.0       165.0  3693.0          11.5    70   \n","2  18.0          8         318.0       150.0  3436.0          11.0    70   \n","3  16.0          8         304.0       150.0  3433.0          12.0    70   \n","4  17.0          8         302.0       140.0  3449.0          10.5    70   \n","\n","   origin  cyl_3  cyl_4  cyl_5  cyl_6  cyl_8  \n","0       1      0      0      0      0      1  \n","1       1      0      0      0      0      1  \n","2       1      0      0      0      0      1  \n","3       1      0      0      0      0      1  \n","4       1      0      0      0      0      1  "]},"metadata":{"tags":[]},"execution_count":41}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"XGU7dwEZYUIZ"},"source":["## 3.3 Multiclass classification"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"7W2JjPtbZFYn"},"source":["In previous sections, we explored **binary classification**, where there were only two possible categories, or classes. When we have three or more categories, we call the problem a **multiclass classification problem**. There are a few different methods of doing multiclass classification and in this section, we'll focus on the **one-versus-all method**.\n","\n","The **one-versus-all** method is a technique where we choose a single category as the Positive case and group the rest of the categories as the False case. **We're essentially splitting the problem into multiple binary classification problems.** For each observation, the model will then output the probability of belonging to each category.\n","\n","To start let's split our data into a training and test set. We've randomized the **cars** Dataframe for you already to start things off and assigned the shuffled Dataframe to **shuffled_cars**.\n","\n","\n","**Exercise Start**\n","\n","<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n","\n","- Split the **shuffled_cars** Dataframe into two Dataframes: **train** and **test**.\n","  - Assign the first 70% of the **shuffled_cars** to **train.**\n","  - Assign the last 30% of the **shuffled_cars** to **test**."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"2iBuxy8tZsej","colab":{}},"source":["shuffled_rows = np.random.permutation(cars.index)\n","shuffled_cars = cars.iloc[shuffled_rows]\n","\n","# put your code here"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7Hf_4lICVpVU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":232},"outputId":"204cf9a5-f369-449d-df3b-fdb8dc5e5372","executionInfo":{"status":"error","timestamp":1568675389416,"user_tz":180,"elapsed":699,"user":{"displayName":"Ivanovitch Silva","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBcjji7pCNFgk53T2rHmLlR9pgrTAc60gzJQYMZ2A=s64","userId":"06428777505436195303"}}},"source":["import numpy as np\n","shuffled_rows = np.random.permutation(cars.index)\n","shuffled_cars = cars.iloc[shuffled_rows]\n","highest_train_row = int(cars.shape[0] * .70)\n","train = shuffled_cars.iloc[0:highest_train_row]\n","test = shuffled_cars.iloc[highest_train_row:]"],"execution_count":2,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-a5c46019c130>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mshuffled_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mshuffled_cars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mshuffled_rows\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mhighest_train_row\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m.70\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshuffled_cars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mhighest_train_row\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'cars' is not defined"]}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"3mdQbGcRaoTI"},"source":["## 3.4 Training a multiclass logistic regression model"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"OIgx8OFea0Dk"},"source":["In the **one-vs-all** approach, we're essentially **converting an n-class** (in our case n is 3) classification problem **into n binary classification problems**. For our case, we'll need to train 3 models:\n","\n","- A model where all cars built in **North America** are considered **Positive (1)** and those built in **Europe** and **Asia** are considered **Negative (0)**.\n","- A model where all cars built in **Europe** are considered **Positive (1)** and those built in **North America** and **Asia** are considered **Negative (0)**.\n","- A model where all cars built in **Asia** are labeled **Positive (1)** and those built in **North America** and **Europe** are considered **Negative (0)**.\n","\n","**Each of these models is a binary classification model** that will return a probability between 0 and 1. When we apply this model on new data, a probability value will be returned from each model (3 total). **For each observation, we choose the label corresponding to the model that predicted the highest probability.**\n","\n","We'll use the dummy variables we created from the **cylinders** and **year** columns to train 3 models using the LogisticRegression class from scikit-learn.\n","\n","**Exercise Start**\n","\n","<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n","\n","- For each value in **unique_origins**, train a **logistic regression** model with the following parameters:\n","  - X: Dataframe containing just the **cylinder** & **year** binary columns.\n","  - y: list (or Series) of Boolean values:\n","    - **True** if observation's value for **origin** matches the current iterator variable.\n","    - **False** if observation's value for **origin** doesn't match the current iterator variable.\n","- Add each model to the models dictionary with the following structure:\n","  - **key**: origin value (1, 2, or 3),\n","  - **value**: relevant **LogistcRegression model** instance."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"fjMkBCjUbOdA","colab":{}},"source":["from sklearn.linear_model import LogisticRegression\n","\n","unique_origins = cars[\"origin\"].unique()\n","unique_origins.sort()\n","\n","models = {}\n","\n","# put your code here"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"cBW4-kCId9-1"},"source":["## 3.5 Testing the models\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"IWKLoUCLfiAg"},"source":["Now that we have a model for each category, we can run our test dataset through the models and evaluate how well they performed.\n","\n","**Exercise Start**\n","\n","<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n","\n","- For each **origin** value from **unique_origins**:\n","  - Use the **LogisticRegression predict_proba** function to return the three lists of predicted probabilities for the **test set** and add to the **testing_probs** Dataframe."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"jqZuBhY_hHp2","colab":{}},"source":["testing_probs = pd.DataFrame(columns=unique_origins)\n","# put your code here"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"HpuI3rs-hoJ3"},"source":["## 3.6 Choose the origin"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"zFk_OZ_nh5lN"},"source":["Now that we trained the models and computed the probabilities in each origin we can classify each observation. **To classify each observation we want to select the origin with the highest probability of classification for that observation.**\n","\n","While each column in our dataframe **testing_probs** represents an origin we just need to choose the one with the largest probability. We can use the Dataframe method [idxmax()](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.idxmax.html) to return a Series where each value corresponds to the column or where the maximum value occurs for that observation. We need to make sure to set the axis paramater to 1 since we want to calculate the maximum value across columns. Since each column maps directly to an origin the resulting Series will be the classification from our model.\n","\n","**Exercise Start**\n","\n","<left><img width=\"100\" alt=\"creating a repo\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\"></left>\n","\n","\n","- Classify each observation in the test set using the **testing_probs** Dataframe.\n","- Assign the predicted origins to **predicted_origins** and use the **print** function to display it.\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"mwz2cPvci8Sp","colab":{}},"source":["# put your code here"],"execution_count":0,"outputs":[]}]}